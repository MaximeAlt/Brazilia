{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8081436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting translate-api\n",
      "  Downloading translate_api-4.9.5-py3-none-any.whl (20 kB)\n",
      "Collecting PyExecJS>=1.5.1\n",
      "  Downloading PyExecJS-1.5.1.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from translate-api) (4.8.0)\n",
      "Requirement already satisfied: requests>=2.25.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from translate-api) (2.27.1)\n",
      "Collecting pathos>=0.2.7\n",
      "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.8/79.8 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting loguru>=0.4.1\n",
      "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting win32-setctime>=1.0.0\n",
      "  Downloading win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from loguru>=0.4.1->translate-api) (0.4.4)\n",
      "Collecting pox>=0.3.2\n",
      "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
      "Collecting ppft>=1.7.6.6\n",
      "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.8/52.8 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting dill>=0.3.6\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting multiprocess>=0.70.14\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.9/132.9 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from PyExecJS>=1.5.1->translate-api) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.25.1->translate-api) (1.26.9)\n",
      "Building wheels for collected packages: PyExecJS\n",
      "  Building wheel for PyExecJS (setup.py): started\n",
      "  Building wheel for PyExecJS (setup.py): finished with status 'done'\n",
      "  Created wheel for PyExecJS: filename=PyExecJS-1.5.1-py3-none-any.whl size=14598 sha256=762c7983480dea8167e3819b1e975fec93a5fd44bc661d5438b9f2457df2be16\n",
      "  Stored in directory: c:\\users\\maxime.alter\\appdata\\local\\pip\\cache\\wheels\\02\\68\\54\\e702279a4359428d51796ed57b9be98018f60d3981361023c3\n",
      "Successfully built PyExecJS\n",
      "Installing collected packages: win32-setctime, PyExecJS, ppft, pox, dill, multiprocess, loguru, pathos, translate-api\n",
      "Successfully installed PyExecJS-1.5.1 dill-0.3.6 loguru-0.6.0 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 translate-api-4.9.5 win32-setctime-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install translate-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbb4ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using France server backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path \n",
    "import missingno as msno\n",
    "from tabulate import tabulate\n",
    "from statistics import median, mean, quantiles\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopy.distance\n",
    "import folium\n",
    "import geopy.distance\n",
    "import translators as ts\n",
    "plt.style.use('ggplot')\n",
    "#pd.set_option('max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4866f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DF pour afficher les stats des données\n",
    "def tstats (data) :\n",
    "    output = []\n",
    "    for col in data.columns:\n",
    "    \n",
    "        nonNull  = len(data) - np.sum(pd.isna(data[col]))\n",
    "        nonNullprop = (nonNull / len(data[col]))*100\n",
    "        unique = data[col].nunique()\n",
    "        colType = str(data[col].dtype)\n",
    "        output.append([col, colType, nonNull, round(nonNullprop, 1) , unique])\n",
    "            \n",
    "    \n",
    "    df_stats = pd.DataFrame(output)\n",
    "    df_stats.columns = ['nom colonne','dtype', 'valeur non null',\"% de non null\", 'nb_unique']\n",
    "\n",
    "    print(tabulate(df_stats, headers='keys', tablefmt='psql'))\n",
    "    \n",
    "    \n",
    "def diff_in_hours (date1, date2) :\n",
    "    result_hour = (date1 - date2) / pd.Timedelta('1 hour')\n",
    "    return result_hour\n",
    "\n",
    "\n",
    "def filter_geo_brazil_lat(latitudes):\n",
    "    filtered_latitudes = []\n",
    "    default_lat = sum(latitudes) / len(latitudes)\n",
    "    for lat in latitudes:\n",
    "         if -33.75 <= lat <= 5.5:\n",
    "            filtered_latitudes.append(lat)\n",
    "         else : \n",
    "            filtered_latitudes.append(default_lat)            \n",
    "    return filtered_latitudes\n",
    "\n",
    "\n",
    "def filter_geo_brazil_lng(longitudes):\n",
    "    filtered_longitudes = []\n",
    "    default_lng = sum(longitudes) / len(longitudes)\n",
    "    for lon in longitudes:\n",
    "        if -74.5 <= lon <= -34.5:\n",
    "            filtered_longitudes.append(lon)\n",
    "        else : \n",
    "            filtered_longitudes.append(default_lng)\n",
    "    return filtered_longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3d55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(os.getcwd()) \n",
    "p_parent = path.parent\n",
    "p_customer = str(p_parent) + '\\data\\olist_customers_dataset.csv'\n",
    "p_geo = str(p_parent) + '\\data\\olist_geolocation_dataset.csv'\n",
    "p_order_item = str(p_parent) + '\\data\\olist_order_items_dataset.csv'\n",
    "p_order_payment = str(p_parent) + '\\data\\olist_order_payments_dataset.csv'\n",
    "p_order_review = str(p_parent) + '\\data\\olist_order_reviews_dataset.csv'\n",
    "p_orders = str(p_parent) + '\\data\\olist_orders_dataset.csv'\n",
    "p_products = str(p_parent) + '\\data\\olist_products_dataset.csv'\n",
    "p_sellers = str(p_parent) + '\\data\\olist_sellers_dataset.csv'\n",
    "p_category = str(p_parent) + '\\data\\product_category_name_translation_expanded.csv'\n",
    "\n",
    "customer = pd.read_csv(p_customer)\n",
    "geo = pd.read_csv(p_geo)\n",
    "order_item = pd.read_csv(p_order_item)\n",
    "order_payment = pd.read_csv(p_order_payment)\n",
    "order_review = pd.read_csv(p_order_review)\n",
    "orders = pd.read_csv(p_orders)\n",
    "products = pd.read_csv(p_products)\n",
    "sellers = pd.read_csv(p_sellers)\n",
    "category = pd.read_csv(p_category)\n",
    "\n",
    "\n",
    "#fusion avec .merge\n",
    "\n",
    "df = orders.merge(customer, on=\"customer_id\", how ='left')\\\n",
    ".merge(order_item, on= \"order_id\", how = 'left')\\\n",
    ".merge(order_payment, on ='order_id', how ='left')\\\n",
    ".merge(order_review, on =\"order_id\", how=\"left\")\\\n",
    ".merge(sellers, on ='seller_id', how =\"left\")\\\n",
    ".merge(products, on = \"product_id\", how = 'left')\\\n",
    ".merge(category, on=\"product_category_name\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41eb44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on va créer un autre df pour traiter ces données :\n",
    "\n",
    "# création de la base des vendeurs :\n",
    "df_geo_seller = sellers.merge(geo, left_on =\"seller_zip_code_prefix\", right_on ='geolocation_zip_code_prefix', how ='inner')\n",
    "#filtre des données hors brezil : \n",
    "df_geo_seller['geolocation_lat'] = filter_geo_brazil_lat(df_geo_seller['geolocation_lat'])\n",
    "df_geo_seller['geolocation_lng'] = filter_geo_brazil_lng(df_geo_seller['geolocation_lng'])\n",
    "#regroupement des données par vendeur : \n",
    "gr_geo_seller = df_geo_seller.groupby(['seller_id'], as_index=False).aggregate({\n",
    "                'geolocation_lat':'mean',\n",
    "                'geolocation_lng' : 'mean',\n",
    "                }).rename(columns={\"geolocation_lat\": \"seller_lat\",\n",
    "                            \"geolocation_lng\": \"seller_lng\",\n",
    "                }) \n",
    "\n",
    "# création de la base des clients :\n",
    "df_geo_customer = customer.merge(geo, left_on =\"customer_zip_code_prefix\", right_on ='geolocation_zip_code_prefix', how ='inner')\n",
    "#filtre des données hors brezil :  \n",
    "df_geo_customer['geolocation_lat'] = filter_geo_brazil_lat(df_geo_customer['geolocation_lat'])\n",
    "df_geo_customer['geolocation_lng'] = filter_geo_brazil_lng(df_geo_customer['geolocation_lng'])\n",
    "#regroupement des données par client unique : \n",
    "gr_geo_customer = df_geo_customer.groupby(['customer_unique_id'], as_index=False).aggregate({\n",
    "                'geolocation_lat':'mean',\n",
    "                'geolocation_lng' : 'mean',\n",
    "                }).rename(columns={\"geolocation_lat\": \"customer_lat\",\n",
    "                            \"geolocation_lng\": \"customer_lng\",\n",
    "                })\n",
    "\n",
    "\n",
    "df = df.merge(gr_geo_seller, left_on =\"seller_id\", right_on ='seller_id', how ='inner')\\\n",
    ".merge(gr_geo_customer, left_on =\"customer_unique_id\", right_on ='customer_unique_id', how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5492a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', 'freight_value', 'payment_sequential', 'payment_type', 'payment_installments', 'payment_value', 'review_id', 'review_score', 'review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp', 'seller_zip_code_prefix', 'seller_city', 'seller_state', 'product_category_name', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm', 'product_category_name_english', 'metacategory', 'seller_lat', 'seller_lng', 'customer_lat', 'customer_lng']\n"
     ]
    }
   ],
   "source": [
    "#création d'un liste des nom de colonnes\n",
    "cols = []\n",
    "\n",
    "for col in df : \n",
    "    cols.append(col)\n",
    "\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d26cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##translate module : cheack https://pypi.org/project/translate-api/\n",
    "\n",
    "#for label, row in df.iterrows() : \n",
    "#    if pd.notna(row['review_comment_title']) | pd.notna(row['review_comment_message']):\n",
    "#        title_trad = ts.google(str(row['review_comment_title']))\n",
    "#        message_trad =  ts.google(row['review_comment_message'])\n",
    "#        df.loc [label, \"titre_trad\"] = title_trad\n",
    "#        df.loc [label, \"message_trad\"] = message_trad\n",
    "#\n",
    "#print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
